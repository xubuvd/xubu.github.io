
#About Me
虚步, a PhD student at BUPT. I am very fortunate to be advised by my advisor. My research is in the area of Vision, Language and Reasoning, with a focus on Visual Dialogue. In particular, I am interested in building a visually-grounded conversational AI (social robot) that can see the world and talk with us in natural language. Other interests include Visual/Language Grounding, Visual Reasoning, Visual Question Generation and Visually-grounded Referring Expression.

Now I'm researching GuessWhich and Visual Dialog(VisDial), please feel free to contact me with pangweitf@bupt.edu.cn or pangweitf@163.com if you have any questions or concerns.

# Current Vision-and-Language-and-Reasoning tasks, focuses on Visual Dialogue
1. Multimodal Dialogs(MMD), AAAI 2018<br>
2. CoDraw, ACL 2019<br>
3. GuessWhich, AAAI 2017<br>
4. Multi-agent GuessWhich, AAMAS 2019<br>
5. GuessWhat?!, CVPR 2017<br>
6. EmbodiedQA, CVPR 2018<br>
7. VideoNavQA, BMVC 2019<br>
8. GuessNumber, SLT 2018<br>
9. VisDial, CVPR 2017<br>
10. Image-Grounded Conversations(IGC), CVPR 2017<br>
11. VDQG, ICCV 2017<br>
12. RDG-Image guessing game, LREC 2014<br>
13. Deal or No Deal, CoRR 2017<br>
14. Video-Grounded Dialogue Systems (VGDS), ACL 2019<br>
15. Vision-Language Navigation (VLN), CVPR 2018<br>
16. Image Captioning<br>
17. Image Retrieval<br>
18. Visually-grounded Referring Expressions<br>
19. Multi-modal Verification, ACL 2019<br>
20. Viual Dialog based Referring Expression<br>
21. VQA<br>
